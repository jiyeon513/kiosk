import React, { useRef, useEffect, useState } from 'react';
import * as faceapi from '@vladmandic/face-api';
import * as tf from '@tensorflow/tfjs';
import * as cocoSsd from '@tensorflow-models/coco-ssd';

function FaceInfoEstimator({ onInfoUpdate, styles }) {
  const videoRef = useRef();
  const [models, setModels] = useState(null);
  const [detectionStatus, setDetectionStatus] = useState("ëª¨ë¸ì„ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤...");

  // 1. ëª¨ë¸ ë¡œë”© ë° ì›¹ìº  ì‹œì‘ (í•œ ë²ˆë§Œ ì‹¤í–‰)
  useEffect(() => {
    const setup = async () => {
      try {
        await tf.setBackend('webgl');
        await tf.ready();
        
        const MODEL_URL = '/models';
        const loadedFaceApiModel = await faceapi.nets.ssdMobilenetv1.loadFromUri(MODEL_URL);
        const loadedCocoSsdModel = await cocoSsd.load();
        
        await Promise.all([
            faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),
            faceapi.nets.ageGenderNet.loadFromUri(MODEL_URL),
        ]);

        setModels({
          faceApi: loadedFaceApiModel,
          objectDetector: loadedCocoSsdModel
        });

        const stream = await navigator.mediaDevices.getUserMedia({ video: {} });
        if (videoRef.current) {
          videoRef.current.srcObject = stream;
        }
        setDetectionStatus("ì¹´ë©”ë¼ë¥¼ ë°”ë¼ë´ ì£¼ì„¸ìš”...");

      } catch (e) {
        console.error("ëª¨ë¸ ë˜ëŠ” ì›¹ìº  ë¡œë”© ì—ëŸ¬:", e);
        setDetectionStatus("ì´ˆê¸°í™” ì‹¤íŒ¨");
      }
    };
    setup();
  }, []);

  // 2. ëª¨ë¸ì´ ë¡œë“œë˜ë©´ ì‹¤ì‹œê°„ ê°ì§€ ì‹œì‘
  useEffect(() => {
    if (!models || !videoRef.current) {
      return;
    }

    const video = videoRef.current;
    
    const detectionInterval = setInterval(async () => {
      if (video.readyState !== 4) return;
      
      const faceDetections = await faceapi
        .detectAllFaces(video, new faceapi.SsdMobilenetv1Options({ minConfidence: 0.5 }))
        .withFaceLandmarks()
        .withAgeAndGender();
      
      const objectDetections = await models.objectDetector.detect(video);
      
      if (onInfoUpdate) {
        const shirtDetected = objectDetections.some(p => p.class === 'tie');
        const peopleInfo = faceDetections.map(d => ({
          age: Math.round(d.age),
          gender: d.gender === 'male' ? 'ë‚¨ì„±' : 'ì—¬ì„±',
        }));
        
        onInfoUpdate({
          count: faceDetections.length,
          isShirtDetected: shirtDetected,
          people: peopleInfo,
        });
      }

    }, 1500); // ê°ì§€ ê°„ê²©ì„ 1.5ì´ˆë¡œ ì¡°ì •í•˜ì—¬ ì•ˆì •ì„± í™•ë³´

    return () => clearInterval(detectionInterval); // ì»´í¬ë„ŒíŠ¸ unmount ì‹œ interval ì •ë¦¬

  }, [models, onInfoUpdate]);

  return (
    <div style={styles.container}>
      <h2 style={styles.title}>ğŸ‘¤ Step 1. ì¹´ë©”ë¼ ë¶„ì„</h2>
      <div style={{...styles.videoContainer, width: '128px', height: '96px', margin: '0 auto'}}>
        <video
          ref={videoRef}
          autoPlay
          muted
          style={styles.video}
        />
      </div>
      <p style={{ textAlign: 'center', color: '#6c757d', marginTop: '10px' }}>
        {detectionStatus}
      </p>
    </div>
  );
}

export default FaceInfoEstimator;
